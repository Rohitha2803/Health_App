{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM8uHrhN6ORL5X2JTQbAnVJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rohitha2803/Health_app/blob/main/Health_ai.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "yFkQ0AjSII0U",
        "outputId": "c0467d98-d61b-4226-a9dd-5d20046095ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.11/dist-packages (1.46.0)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.2.11)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.7.0)\n",
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.11/dist-packages (3.0.1)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.2.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (24.2)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.2.1)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (9.1.2)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.14.0)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.24.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.43.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.6.15)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.25.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install streamlit pyngrok transformers accelerate PyPDF2\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Set your Hugging Face API key here\n",
        "os.environ[\"HF_TOKEN\"] =\"\""
      ],
      "metadata": {
        "id": "-MMMTHSPPXEv"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from PyPDF2 import PdfReader\n",
        "import torch\n",
        "\n",
        "st.set_page_config(page_title=\"ü©∫ Health AI\", layout=\"centered\")\n",
        "\n",
        "# Load model\n",
        "@st.cache_resource\n",
        "def load_model():\n",
        "    model_name = \"ibm-granite/granite-3.3-2b-instruct\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=True)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        use_auth_token=True,\n",
        "        torch_dtype=torch.float16,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "    return tokenizer, model\n",
        "\n",
        "tokenizer, model = load_model()\n",
        "\n",
        "# Extract text from PDF\n",
        "def extract_text_from_pdf(uploaded_file):\n",
        "    reader = PdfReader(uploaded_file)\n",
        "    text = \"\"\n",
        "    for page in reader.pages:\n",
        "        if page.extract_text():\n",
        "            text += page.extract_text()\n",
        "    return text\n",
        "\n",
        "# Sidebar\n",
        "st.sidebar.title(\"üè• Health AI\")\n",
        "page = st.sidebar.radio(\"Choose an Option\", [\"Medical Report\", \"Home Remedies\", \"Medical Chatbot\"])\n",
        "\n",
        "# ---------- Medical Report Analysis ----------\n",
        "if page == \"Medical Report\":\n",
        "    st.title(\" Medical Report Analyzer\")\n",
        "    uploaded_file = st.file_uploader(\"Upload a medical report (PDF)\", type=\"pdf\")\n",
        "\n",
        "    if uploaded_file:\n",
        "        report_text = extract_text_from_pdf(uploaded_file)\n",
        "        st.success(\" Report uploaded.\")\n",
        "\n",
        "        if st.button(\" Analyze Report\"):\n",
        "            with st.spinner(\"Analyzing the medical report...\"):\n",
        "                prompt = f\"\"\"\n",
        "Analyze this medical report and give a detailed summary in a simple way.\n",
        "Also based on the report, generate the health condition.\n",
        "Here is the report:\n",
        "{report_text[:3500]}\n",
        "\"\"\"\n",
        "                inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(model.device)\n",
        "                outputs = model.generate(**inputs, max_new_tokens=768, do_sample=True, temperature=0.7)\n",
        "                result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "                st.subheader(\"ü©∫  Patient Report Summary\")\n",
        "                st.write(result)\n",
        "\n",
        "        st.subheader(\"üí¨ Ask a Question about the Report\")\n",
        "        user_query = st.text_input(\"Ask your question...\")\n",
        "        if user_query:\n",
        "            with st.spinner(\"Thinking...\"):\n",
        "                qa_prompt = f\"Based on this report:\\n{report_text[:3000]}\\n\\nAnswer this question in a simple way: {user_query}\"\n",
        "                inputs = tokenizer(qa_prompt, return_tensors=\"pt\", truncation=True).to(model.device)\n",
        "                outputs = model.generate(**inputs, max_new_tokens=256, temperature=0.7)\n",
        "                answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "                st.markdown(\"**ü§ñ Answer:**\")\n",
        "                st.write(answer)\n",
        "\n",
        "# ---------- Home Remedies ----------\n",
        "elif page == \"Home Remedies\":\n",
        "    st.title(\" Home Remedies & Self-Care Assistant\")\n",
        "    symptoms = st.text_area(\"Enter your symptoms (e.g., fever, cough, sore throat)\", height=100)\n",
        "\n",
        "    if st.button(\" Get Remedies & Self-Care Tips\"):\n",
        "        with st.spinner(\"Fetching personalized suggestions...\"):\n",
        "            prompt = f\"\"\"\n",
        "I have the following symptoms: {symptoms}\n",
        "\n",
        "1. Suggest at least 10 simple and natural home remedies for the symptoms.\n",
        "2. Also provide at least 10 self-care and lifestyle tips to manage or relieve the symptoms.\n",
        "\"\"\"\n",
        "            inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(model.device)\n",
        "            outputs = model.generate(**inputs, max_new_tokens=1024, do_sample=True, temperature=0.8)\n",
        "            result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "            if \"üßò\" in result:\n",
        "                remedies_text, selfcare_text = result.split(\"üßò\", 1)\n",
        "\n",
        "                st.markdown(\"<h3><b>1.  Home Remedies </b></h3>\", unsafe_allow_html=True)\n",
        "                st.write(remedies_text.strip())\n",
        "\n",
        "                st.markdown(\"<h3><b>2.  Self-Care and Lifestyle Tips</b></h3>\", unsafe_allow_html=True)\n",
        "                st.write(selfcare_text.strip())\n",
        "            else:\n",
        "                st.write(result)\n",
        "\n",
        "# ---------- Medical Chatbot ----------\n",
        "elif page == \"Medical Chatbot\":\n",
        "    st.title(\" AI Medical Chatbot\")\n",
        "    st.markdown(\"Ask any general medical question below and get an AI-powered response.\")\n",
        "\n",
        "    # Initialize chat history\n",
        "    if \"chat_history\" not in st.session_state:\n",
        "        st.session_state.chat_history = []\n",
        "\n",
        "    # Chat input\n",
        "    user_query = st.chat_input(\"Ask a medical question...\")\n",
        "\n",
        "    if user_query:\n",
        "        with st.spinner(\"Generating response...\"):\n",
        "            prompt = f\"Answer the following medical question.:\\n\\n{user_query}\"\n",
        "            inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(model.device)\n",
        "            outputs = model.generate(**inputs, max_new_tokens=512, do_sample=True, temperature=0.7)\n",
        "            answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "            # Save interaction\n",
        "            st.session_state.chat_history.append((user_query, answer))\n",
        "\n",
        "    # Display chat history\n",
        "    for user_msg, bot_msg in st.session_state.chat_history:\n",
        "        st.chat_message(\"user\").write(user_msg)\n",
        "        st.chat_message(\"assistant\").write(bot_msg)\n"
      ],
      "metadata": {
        "id": "K_OKfBZDQHLw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}